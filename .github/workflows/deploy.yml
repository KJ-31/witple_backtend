name: CI/CD Pipeline - Backend

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  AWS_REGION: ap-northeast-2
  ECR_REPOSITORY_NAME: witple-backend
  EKS_CLUSTER_NAME: witple-cluster
  EKS_NAMESPACE: witple

permissions:
  id-token: write
  contents: read

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run Tests
      run: |
        echo "Running tests..."
        # python -m pytest (테스트 파일 추가 시)

  build-and-deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    # AWS 자격 증명 구성 (OIDC 방식)
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/github-actions-role
        aws-region: ${{ env.AWS_REGION }}
        role-session-name: github-actions-${{ github.run_id }}
        role-duration-seconds: 3600
    
    # ECR 로그인
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2
    
    # 백엔드 빌드 및 푸시
    - name: Build and push Backend image
      run: |
        docker build -t witple-backend .
        docker tag witple-backend:latest ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY_NAME }}:latest
        docker tag witple-backend:latest ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY_NAME }}:${{ github.sha }}
        docker push ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY_NAME }}:latest
        docker push ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY_NAME }}:${{ github.sha }}
    
    # EKS 설정 및 Access Entries 적용
    - name: Configure EKS access
      run: |
        aws eks update-kubeconfig --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}
        aws sts get-caller-identity
        
        # Check if EKS cluster is accessible via AWS CLI
        echo "Checking EKS cluster accessibility..."
        if aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} > /dev/null 2>&1; then
          echo "EKS cluster is accessible via AWS CLI"
        else
          echo "Cannot access EKS cluster via AWS CLI"
          exit 1
        fi
        
        # Get current AWS identity
        CURRENT_ARN=$(aws sts get-caller-identity --query 'Arn' --output text)
        echo "Current AWS ARN: $CURRENT_ARN"
        
        # Create EKS Access Entry for GitHub Actions role (if not exists)
        echo "Creating EKS Access Entry for GitHub Actions role..."
        aws eks create-access-entry \
          --cluster-name ${{ env.EKS_CLUSTER_NAME }} \
          --region ${{ env.AWS_REGION }} \
          --principal-arn "arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/github-actions-role" \
          --type STANDARD \
          --kubernetes-groups system:masters || {
          echo "Access entry creation failed or already exists"
        }
        
        # Configure kubectl authentication with AWS CLI
        echo "Configuring kubectl authentication..."
        aws eks get-token --cluster-name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}
        
        # Test kubectl access (system:masters 권한으로)
        echo "Testing kubectl access with system:masters permissions..."
        kubectl get nodes
    
    # AWS Load Balancer Controller ServiceAccount 생성
    - name: Create AWS Load Balancer Controller ServiceAccount
      run: |
        cat > aws-load-balancer-controller-sa.yaml << EOF
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: aws-load-balancer-controller
          namespace: kube-system
          annotations:
            eks.amazonaws.com/role-arn: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/aws-load-balancer-controller
        EOF
        kubectl apply -f aws-load-balancer-controller-sa.yaml --validate=false
    
    # Helm 설치
    - name: Install Helm
      run: |
        curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
        helm version
    
    # AWS Load Balancer Controller 설치
    - name: Install AWS Load Balancer Controller
      run: |
        # Helm을 사용하여 AWS Load Balancer Controller 설치
        helm repo add eks https://aws.github.io/eks-charts
        helm repo update
        
        # VPC ID 가져오기
        VPC_ID=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --query 'cluster.resourcesVpcConfig.vpcId' --output text)
        echo "VPC ID: $VPC_ID"
        
        # AWS Load Balancer Controller 설치 (기존 설치가 있다면 업그레이드)
        helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
          -n kube-system \
          --set clusterName=${{ env.EKS_CLUSTER_NAME }} \
          --set serviceAccount.create=false \
          --set serviceAccount.name=aws-load-balancer-controller \
          --set region=${{ env.AWS_REGION }} \
          --set vpcId=$VPC_ID \
          --wait --timeout=300s
        
        # Controller가 준비될 때까지 대기
        kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=aws-load-balancer-controller -n kube-system --timeout=300s
    
    # 환경 변수 설정 (개선된 버전)
    - name: Set environment variables
      run: |
        echo "ECR_REGISTRY=${{ steps.login-ecr.outputs.registry }}" >> $GITHUB_ENV
        echo "ECR_REPOSITORY_NAME=${{ env.ECR_REPOSITORY_NAME }}" >> $GITHUB_ENV
        echo "SECRET_KEY=${{ secrets.SECRET_KEY }}" >> $GITHUB_ENV
        echo "ALLOWED_ORIGINS=${{ secrets.ALLOWED_ORIGINS }}" >> $GITHUB_ENV
        
        # DOMAIN_NAME 처리 (비어있으면 ALB DNS만 사용)
        DOMAIN_NAME="${{ secrets.DOMAIN_NAME }}"
        if [ -z "$DOMAIN_NAME" ] || [ "$DOMAIN_NAME" = "" ]; then
          echo "DOMAIN_NAME is empty, will use ALB DNS only"
          echo "USE_DOMAIN=false" >> $GITHUB_ENV
          echo "DOMAIN_NAME=" >> $GITHUB_ENV
        else
          echo "DOMAIN_NAME=$DOMAIN_NAME" >> $GITHUB_ENV
          echo "USE_DOMAIN=true" >> $GITHUB_ENV
        fi
        
        # VPC 정보 가져오기 (Ingress용)
        VPC_ID=$(aws eks describe-cluster --name ${{ env.EKS_CLUSTER_NAME }} --query 'cluster.resourcesVpcConfig.vpcId' --output text)
        PUBLIC_SUBNET_IDS=$(aws ec2 describe-subnets --filters "Name=tag:Name,Values=*public*" "Name=vpc-id,Values=$VPC_ID" --query 'Subnets[*].SubnetId' --output text | tr '\t' ',')
        ALB_SECURITY_GROUP_ID=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=*alb*" "Name=vpc-id,Values=$VPC_ID" --query 'SecurityGroups[0].GroupId' --output text)
        
        echo "VPC_ID=$VPC_ID" >> $GITHUB_ENV
        echo "PUBLIC_SUBNET_IDS=$PUBLIC_SUBNET_IDS" >> $GITHUB_ENV
        echo "ALB_SECURITY_GROUP_ID=$ALB_SECURITY_GROUP_ID" >> $GITHUB_ENV
        
        # Base64 인코딩
        echo "DATABASE_URL_BASE64=$(echo -n '${{ secrets.DATABASE_URL }}' | base64)" >> $GITHUB_ENV
        echo "REDIS_URL_BASE64=$(echo -n '${{ secrets.REDIS_URL }}' | base64)" >> $GITHUB_ENV
    
    # 데이터베이스 시크릿 생성
    - name: Create database secret
      run: |
        echo "Creating database secret..."
        
        # kubectl을 사용하여 직접 시크릿 생성 (기존 시크릿이 있으면 삭제 후 재생성)
        kubectl delete secret db-secret -n ${{ env.EKS_NAMESPACE }} --ignore-not-found=true
        
        kubectl create secret generic db-secret \
          --namespace=${{ env.EKS_NAMESPACE }} \
          --from-literal=DATABASE_URL="${{ secrets.DATABASE_URL }}" \
          --from-literal=REDIS_URL="${{ secrets.REDIS_URL }}"
        
        echo "Secret created successfully using kubectl"
        
        # 시크릿 확인
        kubectl get secret db-secret -n ${{ env.EKS_NAMESPACE }} -o yaml
    
    # Kubernetes 매니페스트 적용 (개선된 버전)
    - name: Deploy to EKS
      run: |
        # 환경 변수 치환
        envsubst < k8s/deployment.yaml > k8s/deployment-generated.yaml
        envsubst < k8s/configmap.yaml > k8s/configmap-generated.yaml
        
        # Ingress 생성 (도메인 사용 여부에 따라 다르게 처리)
        echo "=== Ingress Generation Debug ==="
        echo "USE_DOMAIN: $USE_DOMAIN"
        echo "DOMAIN_NAME: $DOMAIN_NAME"
        echo "PUBLIC_SUBNET_IDS: $PUBLIC_SUBNET_IDS"
        echo "ALB_SECURITY_GROUP_ID: $ALB_SECURITY_GROUP_ID"
        
        if [ "$USE_DOMAIN" = "true" ]; then
          echo "Creating Ingress with domain: $DOMAIN_NAME"
          envsubst < k8s/ingress.yaml > k8s/ingress-generated.yaml
          echo "Ingress file created from template"
        else
          echo "Creating Ingress without domain (ALB DNS only)"
          cat > k8s/ingress-generated.yaml << 'EOF'
        apiVersion: networking.k8s.io/v1
        kind: Ingress
        metadata:
          name: witple-backend-ingress
          namespace: witple
          annotations:
            kubernetes.io/ingress.class: "alb"
            alb.ingress.kubernetes.io/scheme: "internet-facing"
            alb.ingress.kubernetes.io/target-type: "ip"
            alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}]'
            alb.ingress.kubernetes.io/group.name: "witple-backend"
            alb.ingress.kubernetes.io/group.order: "1"
            alb.ingress.kubernetes.io/healthcheck-path: "/api/v1/health"
            alb.ingress.kubernetes.io/healthcheck-port: "80"
            alb.ingress.kubernetes.io/healthcheck-protocol: "HTTP"
            alb.ingress.kubernetes.io/success-codes: "200"
        EOF
          # 환경 변수 치환으로 서브넷과 보안 그룹 추가
          cat >> k8s/ingress-generated.yaml << EOF
            alb.ingress.kubernetes.io/subnets: "${PUBLIC_SUBNET_IDS}"
            alb.ingress.kubernetes.io/security-groups: "${ALB_SECURITY_GROUP_ID}"
        spec:
          ingressClassName: alb
          rules:
          # ALB DNS로만 접근 (도메인 없음)
          - http:
              paths:
              - path: /
                pathType: Prefix
                backend:
                  service:
                    name: witple-backend-service
                    port:
                      number: 80
        EOF
        fi
        
        # Ingress 파일 생성 확인
        echo "=== Ingress File Check ==="
        if [ -f "k8s/ingress-generated.yaml" ]; then
          echo "✅ Ingress file created successfully"
          echo "File size: $(wc -c < k8s/ingress-generated.yaml) bytes"
          echo "First 10 lines of Ingress file:"
          head -10 k8s/ingress-generated.yaml
        else
          echo "❌ Ingress file not found!"
          exit 1
        fi
        
        # 배포 (각 단계별로 확인)
        echo "Creating namespace..."
        kubectl apply -f k8s/namespace.yaml
        
        echo "Applying ConfigMap..."
        kubectl apply -f k8s/configmap-generated.yaml
        
        echo "Database secret already created in previous step..."
        
        echo "Deploying backend..."
        kubectl apply -f k8s/deployment-generated.yaml
        kubectl apply -f k8s/service.yaml
        
        echo "Applying Ingress..."
        echo "=== Ingress Apply Debug ==="
        kubectl apply -f k8s/ingress-generated.yaml
        echo "Ingress applied successfully"
        
        # Ingress 상태 확인
        echo "=== Ingress Status Check ==="
        kubectl get ingress -n ${{ env.EKS_NAMESPACE }}
        echo "Ingress details:"
        kubectl describe ingress witple-backend-ingress -n ${{ env.EKS_NAMESPACE }}
        
        # 배포 상태 확인
        echo "Waiting for backend deployment to be ready..."
        kubectl rollout status deployment/witple-backend -n ${{ env.EKS_NAMESPACE }} --timeout=300s
        
        echo "Checking backend pods status..."
        kubectl get pods -n ${{ env.EKS_NAMESPACE }} -l app=witple-backend
        
        # Ingress 상태 확인 및 ALB DNS 출력 (개선된 대기 로직)
        echo "Waiting for Ingress to be ready..."
        echo "Checking Ingress status every 10 seconds for up to 10 minutes..."
        
        # 최대 10분까지 대기 (10초마다 체크)
        for i in {1..60}; do
          echo "Attempt $i/60: Checking Ingress status..."
          
          # Ingress 상태 확인
          INGRESS_STATUS=$(kubectl get ingress witple-backend-ingress -n ${{ env.EKS_NAMESPACE }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          
          if [ -n "$INGRESS_STATUS" ]; then
            echo "✅ Ingress is ready! ALB DNS: $INGRESS_STATUS"
            break
          fi
          
          if [ $i -eq 60 ]; then
            echo "❌ Ingress failed to become ready after 10 minutes"
            echo "=== Debug Information ==="
            kubectl describe ingress witple-backend-ingress -n ${{ env.EKS_NAMESPACE }}
            kubectl get events -n ${{ env.EKS_NAMESPACE }} --sort-by='.lastTimestamp'
            exit 1
          fi
          
          echo "⏳ Ingress not ready yet, waiting 10 seconds..."
          sleep 10
        done
        
        # ALB DNS 이름 출력
        ALB_DNS=$(kubectl get ingress witple-backend-ingress -n ${{ env.EKS_NAMESPACE }} -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
        echo "ALB DNS Name: $ALB_DNS"
        
        # ALB DNS를 환경 변수로 설정
        echo "API_URL=http://$ALB_DNS" >> $GITHUB_ENV
        echo "ALB_DNS_NAME=$ALB_DNS" >> $GITHUB_ENV
        
        # 도메인 설정에 따른 URL 출력
        if [ "$USE_DOMAIN" = "true" ]; then
          echo "Backend API URL: https://$DOMAIN_NAME"
          echo "ALB Direct Access: http://$ALB_DNS"
          echo "Health Check: https://$DOMAIN_NAME/api/v1/health"
        else
          echo "Backend API URL: http://$ALB_DNS"
          echo "Health Check: http://$ALB_DNS/api/v1/health"
          echo "Note: No custom domain configured. Access via ALB DNS only."
        fi
